{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import codecs\n",
    "from sklearn import feature_extraction\n",
    "import mpld3 #pip install mpld3\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stopwords, stemming and tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 titles\n",
      "100 links\n",
      "100 synopses_wiki\n",
      "100 genres\n"
     ]
    }
   ],
   "source": [
    "#import three lists: titles, links and wikipedia synopses\n",
    "titles = open('./document_cluster_data/title_list.txt').read().split('\\n')\n",
    "\n",
    "#ensure that only the first 100 are read in\n",
    "titles = titles[:100]\n",
    "\n",
    "links = open('./document_cluster_data/link_list_imdb.txt').read().split('\\n')\n",
    "links = links[:100]\n",
    "\n",
    "synopses_wiki = open('./document_cluster_data/synopses_list_wiki.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_wiki = synopses_wiki[:100]\n",
    "\n",
    "synopses_clean_wiki = []\n",
    "for text in synopses_wiki:\n",
    "    text = BeautifulSoup(text, 'html.parser').getText()\n",
    "    # strips html formatting and converts to unicode\n",
    "    synopses_clean_wiki.append(text)\n",
    "\n",
    "synopses_wiki = synopses_clean_wiki\n",
    "\n",
    "genres = open('./document_cluster_data/genres_list.txt').read().split('\\n')\n",
    "genres = genres[:100]\n",
    "\n",
    "print(str(len(titles)) + ' titles')\n",
    "print(str(len(links)) + ' links')\n",
    "print(str(len(synopses_wiki)) + ' synopses_wiki')\n",
    "print(str(len(genres)) + ' genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "synopses_imdb = open('./document_cluster_data/synopses_list_imdb.txt').read().split('\\n BREAKS HERE')\n",
    "synopses_imdb = synopses_imdb[:100]\n",
    "\n",
    "synopses_clean_imdb = []\n",
    "\n",
    "for text in synopses_imdb:\n",
    "    text = BeautifulSoup(text,'html.parser').getText()\n",
    "    #strips html formatting and converts to unicode\n",
    "    synopses_clean_imdb.append(text)\n",
    "\n",
    "synopses_imdb = synopses_clean_imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "synopses = []\n",
    "\n",
    "for i in range(len(synopses_wiki)):\n",
    "    item = synopses_wiki[i] + synopses_imdb[i]\n",
    "    synopses.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generates index for each item in the corpora (in this case it's just rank) and I'll use this for scoring later\n",
    "ranks = []\n",
    "\n",
    "for i in range(0,len(titles)):\n",
    "    ranks.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is focused on defining some functions to manipulate the synopses. First, I load NLTK's list of English stop words. Stop words are words like \"a\", \"the\", or \"in\" which don't convey significant meaning. I'm sure there are much better explanations of this out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I import the Snowball Stemmer which is actually part of NLTK. Stemming is just the process of breaking a word down into its root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load nltk's SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
